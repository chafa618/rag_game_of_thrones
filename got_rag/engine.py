import annoy
from sentence_transformers import SentenceTransformer
import json
#from chatbot import get_answer_from_local_model, get_answer_from_openai
from chat_completions import get_answer_from_ollama, get_answer_from_openai
import logging


model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')


def get_embeddings(text):
    return model.encode(text)


def load_data(path):
    with open(path, 'r', encoding='utf-8') as json_file:
        json_data = json.load(json_file)
        
    mapping = make_mapping(json_data)
    return json_data, mapping


def make_mapping(json_data):
    chunk_id_mapping = {}
    for chunk in json_data:
        chunk_id_mapping[chunk["chunk_id"]] = chunk
    return chunk_id_mapping


def load_index(path, emb_size):
    index = annoy.AnnoyIndex(emb_size, 'angular')
    index.load(path)
    return index


def process_query(query, index, chunk_id_mapping):
    embedding_pregunta = get_embeddings(query)
    ids_potenciales_respuestas = index.get_nns_by_vector(embedding_pregunta, 5)
    potenciales_respuestas = [chunk_id_mapping[idx] for idx in ids_potenciales_respuestas]
    texto_potencial = [chunk for chunk in potenciales_respuestas]
    return texto_potencial, potenciales_respuestas


def run(query, index, chunk_id_mapping, model_type='local'):
    _, candidatos = process_query(query, index, chunk_id_mapping)
    #print("#####\n\n", _, "\n\n#####")
    if model_type == 'local':
        llm_respuesta = get_answer_from_ollama(query, candidatos)
    else:
        llm_respuesta = get_answer_from_openai(query, candidatos)
    return llm_respuesta, candidatos


# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def run_session(index, chunk_id_mapping, model_type='local'):
    """
    This module contains the engine for running a session in the Game of Thrones RAG (Retrieval-Augmented Generation) application.
    Functions:
        run_session(index, chunk_id_mapping, model_type='local'):
            Runs an interactive session where the user can input queries and receive responses generated by the model.
            Logs the received queries, generated responses, and candidate chunks.
    Parameters:
        index (Any): The index used for retrieving relevant chunks.
        chunk_id_mapping (dict): A mapping of chunk IDs to their corresponding data.
        model_type (str, optional): The type of model to use for generating responses. Defaults to 'local'.
    Returns:
        None
    """
    while True:
        query = input("Introduce tu pregunta. Escribe /bye para salir: ")
        if query == "/bye":
            break
        logging.info(f"Received query: {query}")
        llm_respuesta, candidatos = run(query, index, chunk_id_mapping, model_type)

        logging.info("Generated response:")
        logging.info(llm_respuesta)
        logging.info("Candidate chunks:")
        for candidato in candidatos:
            logging.info(candidato)

if __name__ == '__main__':
    # Load data and index
    _, chunk_id_mapping = load_data('../data/jdt_chunks_sentences_512.json')
    index = load_index('index_juego_de_tronos_chunk_512.ann', 768)
    
    # Configure logging to save to a file
    logging.basicConfig(filename='llm_responses.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # Start interactive session
    while True:
        query = input("Introduce tu pregunta. Escribe /bye para salir: ")
        if query == "/bye":
            break
        
        logging.info(f"Received query: {query}")
        
        # Run local model
        local_llm_respuesta, local_candidatos = run(query, index, chunk_id_mapping, 'local')
        logging.info("Generated response from local model:")
        logging.info(local_llm_respuesta)
        
        # Run OpenAI model
        #openai_llm_respuesta, openai_candidatos = run(query, index, chunk_id_mapping, 'openai')
        #logging.info("Generated response from OpenAI model:")
        #logging.info(openai_llm_respuesta)
        